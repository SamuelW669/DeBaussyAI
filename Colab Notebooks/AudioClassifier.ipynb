{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1RNqUQw_MKuNda2EyYIVsqWRjQsWXWiwR","authorship_tag":"ABX9TyNB9ZqV+566dp0CDOxveSYl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ImhE4jWlhSHQ","executionInfo":{"status":"ok","timestamp":1724002625300,"user_tz":240,"elapsed":1487,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}},"outputId":"aa8de9fe-f1df-426d-9d41-a783b0a161c1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import librosa\n","import numpy as np\n","import torch\n","from sklearn.model_selection import train_test_split\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt"],"metadata":{"id":"D3Du_kuVgpEs","executionInfo":{"status":"ok","timestamp":1724001920810,"user_tz":240,"elapsed":1876,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"kLRpJMEvmIEE"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"vd5EmudefJre","executionInfo":{"status":"ok","timestamp":1724001921704,"user_tz":240,"elapsed":896,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class AudioClassifierLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers):\n","        super(AudioClassifierLSTM, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        # LSTM layer\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(hidden_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, 1)  # Binary classification\n","\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n","\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = out[:, -1, :]  # Take the last output\n","        out = self.relu(self.fc1(out))\n","        out = self.relu(self.fc2(out))\n","        out = self.fc3(out)\n","        out = self.sigmoid(out)  # Output between 0 and 1\n","        return out\n","\n","# Initialize the model\n","input_dim = 33  # Based on your feature extraction\n","hidden_dim = 128\n","num_layers = 2\n","model = AudioClassifierLSTM(input_dim, hidden_dim, num_layers)\n","\n","# Define loss and optimizer\n","criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","def train_model(model, train_loader, val_loader, epochs=10, lr=0.001, graph=False, save_path='model.pth'):\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        epoch_train_loss = 0\n","        for inputs, labels in train_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_train_loss += loss.item()\n","\n","        train_losses.append(epoch_train_loss / len(train_loader))\n","\n","        model.eval()\n","        epoch_val_loss = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                outputs = model(inputs)\n","                loss = criterion(outputs.squeeze(), labels.float())\n","                epoch_val_loss += loss.item()\n","        val_losses.append(epoch_val_loss / len(val_loader))\n","\n","        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}')\n","\n","    # Plotting the training and validation losses\n","    if graph:\n","        plt.figure(figsize=(10, 5))\n","        plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n","        plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.title('Training and Validation Loss')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n","\n","    torch.save(model.state_dict(), save_path)"],"metadata":{"id":"I3OQcGrmfQ0p","executionInfo":{"status":"ok","timestamp":1724002673942,"user_tz":240,"elapsed":236,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs)\n","            preds = (outputs.squeeze() > 0.5).long()  # Convert probabilities to class labels\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    print(confusion_matrix(all_labels, all_preds))\n","    print(classification_report(all_labels, all_preds))\n"],"metadata":{"id":"5_j13UUbfU1h","executionInfo":{"status":"ok","timestamp":1724001921704,"user_tz":240,"elapsed":7,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def extract_features(file_path):\n","    # Load the audio file\n","    y, sr = librosa.load(file_path, sr=None)\n","\n","    # Extract MFCCs\n","    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n","\n","    # Extract Chroma\n","    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n","\n","    # Extract Spectral Contrast\n","    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n","\n","    # Onset detection (to calculate note durations)\n","    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)\n","    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n","\n","    # Calculate durations between onsets\n","    note_durations = np.diff(onset_times)  # Durations between onsets\n","    if len(note_durations) > 0:\n","        avg_note_duration = np.mean(note_durations)  # Average note duration\n","    else:\n","        avg_note_duration = 0  # Handle the case of no detected onsets\n","\n","    # Combine all features into a single feature vector\n","    features = np.concatenate((\n","        mfccs.mean(axis=1), # Timbre\n","        chroma.mean(axis=1), # Pitch Classes (ex: C#2)\n","        spectral_contrast.mean(axis=1), # Contrast between peaks and valleys of frequency\n","        np.array([avg_note_duration])  # Add the average note duration as a feature\n","    ))\n","\n","    return features\n"],"metadata":{"id":"_4vImkDGhHKK","executionInfo":{"status":"ok","timestamp":1724001921705,"user_tz":240,"elapsed":6,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Create a custom dataset class\n","class AudioDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        self.labels = torch.tensor(labels, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        return self.features[idx], self.labels[idx]"],"metadata":{"id":"UHU40qj0j3YP","executionInfo":{"status":"ok","timestamp":1724002565677,"user_tz":240,"elapsed":5,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import torch\n","from os import walk\n","\n","# Auxilliary Function\n","# Import MIDI & MP3 Files\n","synthesized_path = \"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/\"\n","synthesized_temp = []\n","synthesized_files = []\n","human_path = \"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/\"\n","human_temp = []\n","human_files = []\n","\n","# Parsing through MIDI files\n","for (dir_path, dir_names, file_names) in walk(synthesized_path):\n","  synthesized_temp.extend(file_names)\n","\n","for file in synthesized_temp:\n","  midi_file_path = synthesized_path + file\n","  synthesized_files.append(midi_file_path)\n","\n","print(synthesized_files) # Check\n","\n","# Parsing through Human files\n","for (dir_path, dir_names, file_names) in walk(human_path):\n","  human_temp.extend(file_names)\n","\n","for file in human_temp:\n","  human_file_path = human_path + file\n","  human_files.append(human_file_path)\n","\n","print(human_files) # Check\n","\n","# Initialize lists to store features and labels\n","features_list = []\n","labels_list = []\n","\n","# Process synthesized files\n","for file_path in synthesized_files:\n","    features = extract_features(file_path)\n","    features_list.append(features)\n","    labels_list.append(0)  # Label for synthesized audio\n","\n","# Process human files\n","for file_path in human_files:\n","    features = extract_features(file_path)\n","    features_list.append(features)\n","    labels_list.append(1)  # Label for human audio\n","\n","# Convert lists to numpy arrays\n","X = np.array(features_list)\n","y = np.array(labels_list)\n","\n","# Split into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtxhSEeJfa5g","executionInfo":{"status":"ok","timestamp":1724002247453,"user_tz":240,"elapsed":325754,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}},"outputId":"b477f45d-38d4-4170-fe99-b0fc7951cb65"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_1.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_0.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_2.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_3.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_4.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_5.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_7.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_6.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_8.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_9.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_10.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/MIDI_11.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Solo_Violin_Partita_No._2_in_D_Minor_-_J._S._Bach_BWV_1004.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Silent_Night.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Spring-Four_seasons_vivaldi.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of The_Swan_Violin____C._Saint-Saens.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Solo_Violin_Sonata_No._1_in_G_Minor_-_J._S._Bach_BWV_1001.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Summer_-_Third_movement.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Solo_Violin_Caprice_No._24_in_A_Minor_-_N._Paganini_Op._1_No._24.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Paganiniana_-_Nathan_Milstein.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Swan_lake.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Valse_Sentimentale.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Nocturne_No._20_in_C_minor_for_Violin.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Concerto_in_A_minor_A_Vivaldi.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Czardas.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Ode_to_Joy_-_Violin.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Concert_In_G_Major_RV310_-_Vivaldi_Solo_Violin.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Fur_Elise.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Hungarian_dance_No_5.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Meditation_from_Thais.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Nocturne_Op._9_No._2_for_Violin_Sarasate.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Danse_Macabre.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Bach_Cello_Suite_No._1_For_Violin.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Clair_de_Lune_by_Claude_Debussy.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Canon_in_D__Violin_Solo_.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Air_On_The_G_String_-_Johann_Sebastian_Bach.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of Bach_Cello_Suite_No._1_in_G_Major_BWV_1007_Prelude_for_Violin.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/Copy of MIDI_4.mp3']\n","['/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_1.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_0.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_2.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_3.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_4.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_5.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_6.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_9.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_7.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_8.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_10.mp3', '/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/Human_11.mp3']\n"]}]},{"cell_type":"code","source":["# Create datasets and dataloaders\n","train_dataset = AudioDataset(X_train, y_train)\n","test_dataset = AudioDataset(X_test, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"G0Xzw3eWj7IH","executionInfo":{"status":"ok","timestamp":1724002580482,"user_tz":240,"elapsed":220,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Training\n","\n","save_path = r\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/human-ness_model.pth\"\n","model.to(device)\n","train_model(model, train_loader, test_loader, epochs=10, lr=0.001, graph=True, save_path=save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FoRWvP4egvFN","executionInfo":{"status":"error","timestamp":1724002714528,"user_tz":240,"elapsed":224,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}},"outputId":"b287f81a-82a9-453f-8e82-5fac59622b6d"},"execution_count":14,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a5b6ab56cc66>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/human-ness_model.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-7220640e9c49>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, lr, graph, save_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-331948c4dbee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Take the last output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    901\u001b[0m                         msg = (\"For unbatched 2-D input, hx and cx should \"\n\u001b[1;32m    902\u001b[0m                                f\"also be 2-D but got ({hx[0].dim()}-D, {hx[1].dim()}-D) tensors\")\n\u001b[0;32m--> 903\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                     \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0;31m# Each batch of the hidden state should match the input sequence that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PXEwuxHQkcFl"},"execution_count":null,"outputs":[]}]}
