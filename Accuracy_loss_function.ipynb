{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQiIUwuX0HInQcYKp04ONE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelW669/DeBaussyAI/blob/main/Accuracy_loss_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "A29IkKX3DHVX",
        "outputId": "8d2cbdf8-39a8-4b59-dafb-26b79797315a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file in path/to/your/file.musicxml",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f6f60aaa85bc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load and parse the MusicXML file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path/to/your/file.musicxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Extract the notes and their attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/music21/converter/__init__.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(value, forceSource, number, format, **keywords)\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindFormatFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;31m# assume mistyped file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Cannot find file in {str(value)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# all else, including MidiBytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file in path/to/your/file.musicxml"
          ]
        }
      ],
      "source": [
        "from music21 import converter\n",
        "\n",
        "# Load and parse the MusicXML file\n",
        "score = converter.parse('path/to/your/file.musicxml')\n",
        "\n",
        "# Extract the notes and their attributes\n",
        "notes = []\n",
        "for part in score.parts:\n",
        "    for note in part.notes:\n",
        "        notes.append({\n",
        "            'pitch': note.pitch.midi,  # MIDI number\n",
        "            'start_time': note.offset,  # Start time in quarter lengths\n",
        "            'duration': note.quarterLength  # Duration in quarter lengths\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Load the MP3 file\n",
        "audio = AudioSegment.from_mp3('path/to/your/file.mp3')\n",
        "\n",
        "# Export as WAV\n",
        "audio.export('file.wav', format='wav')\n"
      ],
      "metadata": {
        "id": "yCJZrmMEDdSj",
        "outputId": "cf9e5b0c-74ec-4a32-901a-99ff6267d993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pydub'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3685e7d77f62>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the MP3 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_mp3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path/to/your/file.mp3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Load the WAV file\n",
        "y, sr = librosa.load('file.wav')\n",
        "\n",
        "# Extract pitch (intonation)\n",
        "pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "\n",
        "# Extract onset times (rhythm)\n",
        "onset_frames = librosa.onset.onset_detect(y=y, sr=sr)\n",
        "onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n"
      ],
      "metadata": {
        "id": "uPSm-1mJDmZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_intonation(expected_notes, detected_pitches):\n",
        "    # Compare expected and detected pitches\n",
        "    intonation_results = []\n",
        "    for i, note in enumerate(expected_notes):\n",
        "        detected_pitch = detected_pitches[i]\n",
        "        if abs(note['pitch'] - detected_pitch) < some_threshold:\n",
        "            intonation_results.append(True)\n",
        "        else:\n",
        "            intonation_results.append(False)\n",
        "    return intonation_results\n",
        "\n",
        "def compare_rhythm(expected_notes, onset_times):\n",
        "    # Compare expected and detected onset times\n",
        "    rhythm_results = []\n",
        "    for i, note in enumerate(expected_notes):\n",
        "        detected_onset = onset_times[i]\n",
        "        if abs(note['start_time'] - detected_onset) < some_threshold:\n",
        "            rhythm_results.append(True)\n",
        "        else:\n",
        "            rhythm_results.append(False)\n",
        "    return rhythm_results\n",
        "\n",
        "# Assume `detected_pitches` and `onset_times` are obtained from librosa analysis\n",
        "intonation_results = compare_intonation(notes, detected_pitches)\n",
        "rhythm_results = compare_rhythm(notes, onset_times)\n",
        "\n",
        "# Display the results\n",
        "print(\"Intonation results:\", intonation_results)\n",
        "print(\"Rhythm results:\", rhythm_results)\n"
      ],
      "metadata": {
        "id": "GQzTeELdDn3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install music21 librosa pydub fastdtw numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Sr3kFQF_Tx",
        "outputId": "a25ee69c-00f4-46ca-a9e6-293fa032156d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: music21 in /usr/local/lib/python3.10/dist-packages (9.1.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting fastdtw\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from music21) (5.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from music21) (1.4.2)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.10/dist-packages (from music21) (3.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from music21) (3.7.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from music21) (10.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from music21) (2.31.0)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.10/dist-packages (from music21) (24.6.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (2024.7.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->music21) (1.16.0)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=512614 sha256=9faede1e4a3881ee1845a91b08f7475d84c70714a4f27d8941075875e3254fa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: pydub, fastdtw\n",
            "Successfully installed fastdtw-0.3.4 pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import converter\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import numpy as np\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "# Step 1: Parse the .musicxml file\n",
        "def parse_musicxml(file_path):\n",
        "    score = converter.parse(file_path)\n",
        "    notes = []\n",
        "    for part in score.parts:\n",
        "        for note in part.notes:\n",
        "            notes.append({\n",
        "                'pitch': note.pitch.midi,  # MIDI number\n",
        "                'start_time': note.offset,  # Start time in quarter lengths\n",
        "                'duration': note.quarterLength  # Duration in quarter lengths\n",
        "            })\n",
        "    return notes\n",
        "\n",
        "# Step 2: Convert MP3 to WAV\n",
        "def convert_mp3_to_wav(mp3_path, wav_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio.export(wav_path, format='wav')\n",
        "\n",
        "# Step 3: Analyze the WAV file\n",
        "def analyze_wav(wav_path):\n",
        "    y, sr = librosa.load(wav_path)\n",
        "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)\n",
        "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "\n",
        "    # Extract the pitches at onset times\n",
        "    detected_pitches = []\n",
        "    for onset_time in onset_times:\n",
        "        frame = librosa.time_to_frames(onset_time, sr=sr)\n",
        "        pitch = pitches[:, frame].max()\n",
        "        if pitch > 0:  # Ignore zero pitch values\n",
        "            detected_pitches.append((pitch, onset_time))\n",
        "\n",
        "    return detected_pitches\n",
        "\n",
        "# Step 4: Align detected pitches with expected notes\n",
        "def align_notes(expected_notes, detected_pitches):\n",
        "    expected_pitches = np.array([note['pitch'] for note in expected_notes])\n",
        "    expected_times = np.array([note['start_time'] for note in expected_notes])\n",
        "\n",
        "    detected_pitches_array = np.array([pitch for pitch, time in detected_pitches])\n",
        "    detected_times = np.array([time for pitch, time in detected_pitches])\n",
        "\n",
        "    distance, path = fastdtw(expected_pitches, detected_pitches_array, dist=euclidean)\n",
        "\n",
        "    aligned_pitches = []\n",
        "    for i, j in path:\n",
        "        aligned_pitches.append((expected_times[i], expected_pitches[i], detected_times[j], detected_pitches_array[j]))\n",
        "\n",
        "    return aligned_pitches\n",
        "\n",
        "# Step 5: Compare intonation and rhythm\n",
        "def compare_intonation_and_rhythm(aligned_pitches, intonation_threshold=1.0, rhythm_threshold=0.1): # Thresholds control the tolerance\n",
        "    results = []\n",
        "    for expected_time, expected_pitch, detected_time, detected_pitch in aligned_pitches:\n",
        "        intonation_correct = abs(expected_pitch - detected_pitch) < intonation_threshold\n",
        "        rhythm_correct = abs(expected_time - detected_time) < rhythm_threshold\n",
        "        results.append((expected_time, expected_pitch, detected_time, detected_pitch, intonation_correct, rhythm_correct))\n",
        "    return results\n",
        "\n",
        "# Main program\n",
        "def main():\n",
        "    musicxml_path = 'path/to/your/file.musicxml'\n",
        "    mp3_path = 'path/to/your/file.mp3'\n",
        "    wav_path = 'path/to/your/file.wav'\n",
        "\n",
        "    # Parse MusicXML\n",
        "    expected_notes = parse_musicxml(musicxml_path)\n",
        "\n",
        "    # Convert MP3 to WAV\n",
        "    convert_mp3_to_wav(mp3_path, wav_path)\n",
        "\n",
        "    # Analyze WAV\n",
        "    detected_pitches = analyze_wav(wav_path)\n",
        "\n",
        "    # Align detected pitches with expected notes\n",
        "    aligned_pitches = align_notes(expected_notes, detected_pitches)\n",
        "\n",
        "    # Compare intonation and rhythm\n",
        "    results = compare_intonation_and_rhythm(aligned_pitches)\n",
        "\n",
        "    # Print the results\n",
        "    for result in results:\n",
        "        expected_time, expected_pitch, detected_time, detected_pitch, intonation_correct, rhythm_correct = result\n",
        "        print(f\"Expected: {expected_pitch} at {expected_time}s, Detected: {detected_pitch} at {detected_time}s\")\n",
        "        print(f\"Intonation Correct: {intonation_correct}, Rhythm Correct: {rhythm_correct}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "HKK8iWv8GAKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import converter\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import numpy as np\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "# Parse the .musicxml file\n",
        "def parse_musicxml(file_path):\n",
        "    score = converter.parse(file_path)\n",
        "    notes = []\n",
        "    for part in score.parts:\n",
        "        for note in part.notes:\n",
        "            notes.append({\n",
        "                'pitch': note.pitch.midi,\n",
        "                'start_time': note.offset,\n",
        "                'duration': note.quarterLength\n",
        "            })\n",
        "    return notes\n",
        "\n",
        "# Convert MP3 to WAV\n",
        "def convert_mp3_to_wav(mp3_path, wav_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio.export(wav_path, format='wav')\n",
        "\n",
        "# Detect vibrato by identifying rapid pitch changes\n",
        "def detect_vibrato(pitches, threshold=1.0):\n",
        "    vibrato_indices = []\n",
        "    for i in range(1, len(pitches)):\n",
        "        if abs(pitches[i] - pitches[i - 1]) > threshold:\n",
        "            vibrato_indices.append(i)\n",
        "    return vibrato_indices\n",
        "\n",
        "# Smooth pitches by averaging over a window around vibrato indices\n",
        "def smooth_pitches(pitches, vibrato_indices, window_size=5):\n",
        "    smoothed_pitches = np.copy(pitches)\n",
        "    for i in vibrato_indices:\n",
        "        start = max(0, i - window_size // 2)\n",
        "        end = min(len(pitches), i + window_size // 2 + 1)\n",
        "        window = pitches[start:end]\n",
        "        smoothed_pitches[i] = np.mean(window)\n",
        "    return smoothed_pitches\n",
        "\n",
        "# Analyze the WAV file\n",
        "def analyze_wav(wav_path):\n",
        "    y, sr = librosa.load(wav_path)\n",
        "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)\n",
        "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "\n",
        "    # Extract the pitches at onset times\n",
        "    detected_pitches = []\n",
        "    for onset_time in onset_times:\n",
        "        frame = librosa.time_to_frames(onset_time, sr=sr)\n",
        "        pitch = pitches[:, frame].max()\n",
        "        if pitch > 0:\n",
        "            detected_pitches.append((pitch, onset_time))\n",
        "\n",
        "    # Separate pitch values and times\n",
        "    pitch_values = [pitch for pitch, time in detected_pitches]\n",
        "    vibrato_indices = detect_vibrato(pitch_values)\n",
        "\n",
        "    # Smooth the detected pitches to handle vibrato\n",
        "    smoothed_pitch_values = smooth_pitches(pitch_values, vibrato_indices)\n",
        "\n",
        "    # Reconstruct detected_pitches with smoothed values\n",
        "    detected_pitches = [(smoothed_pitch_values[i], detected_pitches[i][1]) for i in range(len(detected_pitches))]\n",
        "\n",
        "    return detected_pitches\n",
        "\n",
        "# Align detected pitches with expected notes\n",
        "def align_notes(expected_notes, detected_pitches):\n",
        "    expected_pitches = np.array([note['pitch'] for note in expected_notes])\n",
        "    expected_times = np.array([note['start_time'] for note in expected_notes])\n",
        "\n",
        "    detected_pitches_array = np.array([pitch for pitch, time in detected_pitches])\n",
        "    detected_times = np.array([time for pitch, time in detected_pitches])\n",
        "\n",
        "    distance, path = fastdtw(expected_pitches, detected_pitches_array, dist=euclidean)\n",
        "\n",
        "    aligned_pitches = []\n",
        "    for i, j in path:\n",
        "        aligned_pitches.append((expected_times[i], expected_pitches[i], detected_times[j], detected_pitches_array[j]))\n",
        "\n",
        "    return aligned_pitches\n",
        "\n",
        "# Compare intonation and rhythm\n",
        "def compare_intonation_and_rhythm(aligned_pitches, intonation_threshold=1.0, rhythm_threshold=0.1):\n",
        "    results = []\n",
        "    for expected_time, expected_pitch, detected_time, detected_pitch in aligned_pitches:\n",
        "        intonation_correct = abs(expected_pitch - detected_pitch) < intonation_threshold\n",
        "        rhythm_correct = abs(expected_time - detected_time) < rhythm_threshold\n",
        "        results.append((expected_time, expected_pitch, detected_time, detected_pitch, intonation_correct, rhythm_correct))\n",
        "    return results\n",
        "\n",
        "# Main program\n",
        "def main():\n",
        "    musicxml_path = 'path/to/your/file.musicxml'\n",
        "    mp3_path = 'path/to/your/file.mp3'\n",
        "    wav_path = 'path/to/your/file.wav'\n",
        "\n",
        "    # Parse MusicXML\n",
        "    expected_notes = parse_musicxml(musicxml_path)\n",
        "\n",
        "    # Convert MP3 to WAV\n",
        "    convert_mp3_to_wav(mp3_path, wav_path)\n",
        "\n",
        "    # Analyze WAV\n",
        "    detected_pitches = analyze_wav(wav_path)\n",
        "\n",
        "    # Align detected pitches with expected notes\n",
        "    aligned_pitches = align_notes(expected_notes, detected_pitches)\n",
        "\n",
        "    # Compare intonation and rhythm with adjusted tolerances\n",
        "    results = compare_intonation_and_rhythm(aligned_pitches, intonation_threshold=0.5, rhythm_threshold=0.05)\n",
        "\n",
        "    # Print the results\n",
        "    for result in results:\n",
        "        expected_time, expected_pitch, detected_time, detected_pitch, intonation_correct, rhythm_correct = result\n",
        "        print(f\"Expected: {expected_pitch} at {expected_time}s, Detected: {detected_pitch} at {detected_time}s\")\n",
        "        print(f\"Intonation Correct: {intonation_correct}, Rhythm Correct: {rhythm_correct}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "n83dbwc-HaE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import converter\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import numpy as np\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "# Parse the .musicxml file\n",
        "def parse_musicxml(file_path):\n",
        "    score = converter.parse(file_path)\n",
        "    notes = []\n",
        "    for part in score.parts:\n",
        "        for note in part.notes:\n",
        "            notes.append({\n",
        "                'pitch': note.pitch.midi,\n",
        "                'start_time': note.offset,\n",
        "                'duration': note.quarterLength\n",
        "            })\n",
        "    return notes\n",
        "\n",
        "# Convert MP3 to WAV\n",
        "def convert_mp3_to_wav(mp3_path, wav_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio.export(wav_path, format='wav')\n",
        "\n",
        "# Detect vibrato by identifying rapid pitch changes\n",
        "def detect_vibrato(pitches, threshold=1.0):\n",
        "    vibrato_indices = []\n",
        "    for i in range(1, len(pitches)):\n",
        "        if abs(pitches[i] - pitches[i - 1]) > threshold:\n",
        "            vibrato_indices.append(i)\n",
        "    return vibrato_indices\n",
        "\n",
        "# Smooth pitches by averaging over a window around vibrato indices\n",
        "def smooth_pitches(pitches, vibrato_indices, window_size=5):\n",
        "    smoothed_pitches = np.copy(pitches)\n",
        "    half_window = window_size // 2\n",
        "\n",
        "    for i in vibrato_indices:\n",
        "        start = max(0, i - half_window)\n",
        "        end = min(len(pitches), i + half_window + 1)\n",
        "        window = pitches[start:end]\n",
        "        smoothed_pitches[i] = np.mean(window)\n",
        "\n",
        "    return smoothed_pitches\n",
        "\n",
        "# Analyze the WAV file\n",
        "def analyze_wav(wav_path):\n",
        "    y, sr = librosa.load(wav_path)\n",
        "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
        "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)\n",
        "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "\n",
        "    # Extract the pitches at onset times\n",
        "    detected_pitches = []\n",
        "    for onset_time in onset_times:\n",
        "        frame = librosa.time_to_frames(onset_time, sr=sr)\n",
        "        pitch = pitches[:, frame].max()\n",
        "        if pitch > 0:\n",
        "            detected_pitches.append((pitch, onset_time))\n",
        "\n",
        "    # Separate pitch values and times\n",
        "    pitch_values = [pitch for pitch, time in detected_pitches]\n",
        "    vibrato_indices = detect_vibrato(pitch_values)\n",
        "\n",
        "    # Smooth the detected pitches to handle vibrato\n",
        "    smoothed_pitch_values = smooth_pitches(pitch_values, vibrato_indices)\n",
        "\n",
        "    # Reconstruct detected_pitches with smoothed values\n",
        "    detected_pitches = [(smoothed_pitch_values[i], detected_pitches[i][1]) for i in range(len(detected_pitches))]\n",
        "\n",
        "    return detected_pitches\n",
        "\n",
        "# Align detected pitches with expected notes\n",
        "def align_notes(expected_notes, detected_pitches):\n",
        "    expected_pitches = np.array([note['pitch'] for note in expected_notes])\n",
        "    expected_times = np.array([note['start_time'] for note in expected_notes])\n",
        "\n",
        "    detected_pitches_array = np.array([pitch for pitch, time in detected_pitches])\n",
        "    detected_times = np.array([time for pitch, time in detected_pitches])\n",
        "\n",
        "    distance, path = fastdtw(expected_pitches, detected_pitches_array, dist=euclidean)\n",
        "\n",
        "    aligned_pitches = []\n",
        "    for i, j in path:\n",
        "        aligned_pitches.append((expected_times[i], expected_pitches[i], detected_times[j], detected_pitches_array[j]))\n",
        "\n",
        "    return aligned_pitches\n",
        "\n",
        "# Compare intonation and rhythm\n",
        "def compare_intonation_and_rhythm(aligned_pitches, intonation_threshold=1.0, rhythm_threshold=0.1):\n",
        "    results = []\n",
        "    for expected_time, expected_pitch, detected_time, detected_pitch in aligned_pitches:\n",
        "        intonation_correct = abs(expected_pitch - detected_pitch) < intonation_threshold\n",
        "        rhythm_correct = abs(expected_time - detected_time) < rhythm_threshold\n",
        "        results.append((expected_time, expected_pitch, detected_time, detected_pitch, intonation_correct, rhythm_correct))\n",
        "    return results\n",
        "\n",
        "# Main program\n",
        "def main():\n",
        "    musicxml_path = 'path/to/your/file.musicxml'\n",
        "    mp3_path = 'path/to/your/file.mp3'\n",
        "    wav_path = 'path/to/your/file.wav'\n",
        "\n",
        "    # Parse MusicXML\n",
        "    expected_notes = parse_musicxml(musicxml_path)\n",
        "\n",
        "    # Convert MP3 to WAV\n",
        "    convert_mp3_to_wav(mp3_path, wav_path)\n",
        "\n",
        "    # Analyze WAV\n",
        "    detected_pitches = analyze_wav(wav_path)\n",
        "\n",
        "    # Align detected pitches with expected notes\n",
        "    aligned_pitches = align_notes(expected_notes, detected_pitches)\n",
        "\n",
        "    # Compare intonation and rhythm with adjusted tolerances\n",
        "    results = compare_intonation_and_rhythm(aligned_pitches, intonation_threshold=0.5, rhythm_threshold=0.05)\n",
        "\n",
        "    # Print the results\n",
        "    for result in results:\n",
        "        expected_time, expected_pitch, detected_time, detected_pitch, intonation_correct, rhythm_correct = result\n",
        "        print(f\"Expected: {expected_pitch} at {expected_time}s, Detected: {detected_pitch} at {detected_time}s\")\n",
        "        print(f\"Intonation Correct: {intonation_correct}, Rhythm Correct: {rhythm_correct}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "nCW_JEHuHcjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}