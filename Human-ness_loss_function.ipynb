{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","mount_file_id":"18vTeRpXmndkDNJnfP1NaK7CAJNpzVNJb","authorship_tag":"ABX9TyN/ArDkUWOjsmpq2v2YODO0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# import torch\n","# import torchaudio\n","\n","# temp, _ = torchaudio.load('/content/drive/MyDrive/MIDI4STRINGS_but_better/temp.wav')\n","\n","# temp = torch.Tensor(temp)\n","\n","# print(temp.shape)"],"metadata":{"id":"nkxrZ2ca7d4s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","\n","# # Define your LSTM model\n","# class LSTMModel(nn.Module):\n","#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n","#         super(LSTMModel, self).__init__()\n","#         self.hidden_size = hidden_size\n","#         self.num_layers = num_layers\n","#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","#         self.fc = nn.Linear(hidden_size, output_size)\n","#         self.sigmoid = nn.Sigmoid()\n","\n","#     def forward(self, x):\n","#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","\n","#         out, _ = self.lstm(x, (h0, c0))\n","\n","#         out = self.fc(out[:, -1, :])\n","#         out = self.sigmoid(out)\n","#         return out\n","\n","# # Example usage\n","# input_size = (100,2) # define your input size based on your feature extraction\n","# hidden_size = 128\n","# num_layers = 2\n","# output_size = 1\n","\n","# model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n","\n","\n","# # # Define loss and optimizer\n","# # criterion = nn.BCELoss()\n","# # optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# # Example data loading and training loop\n","\n","# def prepare_dataset(human_files, synthesized_files):\n","#     features = []\n","#     labels = []\n","\n","#     for file in human_files:\n","#         features.append(extract_features(file))\n","#         labels.append(0.0)  # 0.0 for human\n","\n","#     for file in synthesized_files:\n","#         features.append(extract_features(file))\n","#         labels.append(1.0)  # 1.0 for synthesized\n","\n","#     features = np.array(features)\n","#     labels = np.array(labels)\n","#     return train_test_split(features, labels, test_size=0.0001, random_state=14)\n","\n","# # Example file lists\n","# human_files = []\n","# synthesized_files = []\n","# for i in range(10):\n","#   human_files.append(f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human_{i}.mp3\")\n","#   synthesized_files.append(f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/MIDI_{i}.mp3\")\n","\n","# # Emphasize lack of importance of silent time at the beginning\n","# for i in range(3):\n","#   synthesized_files.append(f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/MIDI_{4}.mp3\")\n","#   human_files.append(f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human_{5}.mp3\")\n","\n","# X_train, X_test, y_train, y_test = prepare_dataset(human_files, synthesized_files)\n","\n","# # Training the model\n","# def train_model(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001):\n","#     criterion = nn.BCELoss()\n","#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","#     train_losses = []\n","#     test_losses = []\n","\n","#     for epoch in range(epochs):\n","#         model.train()\n","#         optimizer.zero_grad()\n","#         outputs = model(X_train)\n","#         # if (epoch == 1):\n","#         #   print(outputs.shape, y_train.shape)\n","#         loss = criterion(outputs, y_train)\n","#         loss.backward()\n","#         optimizer.step()\n","#         train_losses.append(loss.item()) # Records the losses for visualization\n","\n","#         if (epoch+1) % 100 == 0:\n","#             model.eval()\n","#             test_outputs = model(X_test)\n","#             test_loss = criterion(test_outputs, y_test)\n","#             test_losses.append(test_loss.item())\n","#             print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}')\n","\n","#     return train_losses, test_losses\n","\n","# train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test, epochs=2000, learning_rate=0.0001) # Remove first half of equation if no visualization\n","\n","# # Visualize\n","# def plot_losses(train_losses, test_losses):\n","#     plt.figure(figsize=(10, 5))\n","#     plt.plot(train_losses, label='Train Loss')\n","#     plt.plot(test_losses, label='Test Loss')\n","#     plt.xlabel('Epoch')\n","#     plt.ylabel('Loss')\n","#     plt.title('Training and Test Loss Over Epochs')\n","#     plt.legend()\n","#     plt.grid(True)\n","#     plt.show()\n","\n","# plot_losses(train_losses, test_losses)\n","\n","# # Save the model\n","# def save_model(model, path):\n","#     torch.save(model.state_dict(), path)\n","#     print(f\"Model saved to {path}\")\n","\n","# save_path = \"audio_regressor_model.pth\"\n","# save_model(model, save_path)\n","# --------------------------------------------------------------------------------------------------------------------------------------------------\n","# # Assuming you have a DataLoader `train_loader` containing your data\n","\n","# # num_epochs = 10\n","# # for epoch in range(num_epochs):\n","# #     for i in range(len(X_train)):\n","# #         optimizer.zero_grad()\n","# #         outputs = model(X_train[i])\n","# #         loss = criterion(outputs, y_train[i])\n","# #         loss.backward()\n","# #         optimizer.step()\n","\n","# #     # Validation after each epoch if needed\n","\n","# # # Evaluate your model on test data\n","# # # Assuming you have a separate DataLoader `test_loader` for test data\n","# # model.eval()\n","# # with torch.no_grad():\n","# #     for i in range(len(X_test)):\n","# #         outputs = model(X_test[i])\n","# #         # Calculate metrics (accuracy, etc.) as needed"],"metadata":{"id":"O7fwoHeqzY8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from google.colab import drive\n","drive.mount('/content/drive')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NtfZeB0Hl4J2","executionInfo":{"status":"ok","timestamp":1728917752287,"user_tz":240,"elapsed":15914,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}},"outputId":"ffec164e-79fe-4536-a04f-f3292985897e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Qe1kVOUQrC0a","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1728917876149,"user_tz":240,"elapsed":620,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}},"outputId":"4945a25a-133e-424d-e3ff-b4d01752303b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","[]\n"]},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-522d13695d28>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#   human_files.append(f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human_{0}.mp3\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynthesized_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For test/train split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-522d13695d28>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(human_files, synthesized_files)\u001b[0m\n\u001b[1;32m     29\u001b[0m        \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1.0 for synthesized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Everything is train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m    \u001b[0;31m# features = np.array(features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m    \u001b[0;31m# labels = np.array(labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":[" # Feature extraction and data preparation\n","import librosa\n","import numpy as np\n","import torch\n","from sklearn.model_selection import train_test_split\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","\n","def extract_features(file_path):\n","    y, sr = librosa.load(file_path, sr=None)\n","    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n","    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n","    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n","    features = np.concatenate((mfccs.mean(axis=1), chroma.mean(axis=1), spectral_contrast.mean(axis=1)))\n","    return features\n","\n","def prepare_dataset(human_files, synthesized_files):\n","    features = []\n","    labels = []\n","\n","    for file in human_files:\n","        features.append(extract_features(file))\n","        labels.append(0.0)  # 0.0 for human\n","\n","    for file in synthesized_files:\n","        features.append(extract_features(file))\n","        labels.append(1.0)  # 1.0 for synthesized\n","\n","    return (features[:-1], features[-1], labels[:-1], labels[-1]) # Everything is train\n","    # features = np.array(features)\n","    # labels = np.array(labels)\n","    # return train_test_split(features, labels, test_size=0.00001, random_state=14)\n","\n","# Example file lists\n","import torch\n","from os import walk\n","\n","# Auxilliary Function\n","# Import MIDI & MP3 Files\n","synthesized_path = \"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/MIDI_Synth/\"\n","synthesized_temp = []\n","synthesized_files = []\n","human_path = \"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/Human/\"\n","human_temp = []\n","human_files = []\n","\n","# Parsing through MIDI files\n","for (dir_path, dir_names, file_names) in walk(synthesized_path):\n","  synthesized_temp.extend(file_names)\n","\n","for file in synthesized_temp:\n","  midi_file_path = synthesized_path + file\n","  synthesized_files.append(midi_file_path)\n","\n","print(synthesized_files) # Check\n","\n","# Parsing through Human files\n","for (dir_path, dir_names, file_names) in walk(human_path):\n","  human_temp.extend(file_names)\n","\n","for file in human_temp:\n","  human_file_path = human_path + file\n","  human_files.append(human_file_path)\n","\n","print(human_files) # Check\n","\n","# # Emphasize lack of importance of silent time at the beginning + Human_0 is just so good\n","# for i in range(2):\n","#   synthesized_files.append(f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/MIDI_{4}.mp3\")\n","#   human_files.append(f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human_{5}.mp3\")\n","#   human_files.append(f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human_{0}.mp3\")\n","\n","X_train, X_test, y_train, y_test = prepare_dataset(human_files, synthesized_files) # For test/train split\n","\n","\n","# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n","y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"]},{"cell_type":"code","source":["# Model definition\n","class AudioRegressor(nn.Module):\n","    def __init__(self, input_dim):\n","        super(AudioRegressor, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, 2)\n","        self.fc4 = nn.Linear(2, 32)\n","        self.fc5 = nn.Linear(32, 64)\n","        self.fc6 = nn.Linear(64,1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.relu(self.fc3(x))\n","        x = self.relu(self.fc4(x))\n","        x = self.relu(self.fc5(x))\n","        x = self.fc6(x)\n","        return x\n","\n","input_dim = X_train.shape[1]\n","model = AudioRegressor(input_dim)\n","\n","# Training the model\n","def train_model(model, X_train, y_train, X_test, y_test, epochs=100, learning_rate=0.0001):\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    train_losses = []\n","    test_losses = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train)\n","        # if (epoch == 1):\n","        #   print(outputs.shape, y_train.shape)\n","        loss = criterion(outputs, y_train)\n","        loss.backward()\n","        optimizer.step()\n","        train_losses.append(loss.item()) # Records the losses for visualization\n","\n","        if (epoch+1) % 1 == 0:\n","            model.eval()\n","            test_outputs = model(X_test)\n","            test_loss = criterion(test_outputs, y_test)\n","            test_losses.append(test_loss.item())\n","            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}')\n","\n","    return train_losses, test_losses\n","\n","train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test, epochs=10000, learning_rate=0.00001) # Remove first half of equation if no visualization\n","\n","# Visualize\n","def plot_losses(train_losses, test_losses):\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(test_losses, label='Test Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Training and Test Loss Over Epochs')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","plot_losses(train_losses, test_losses)\n","\n","# Save the model\n","def save_model(model, path):\n","    torch.save(model.state_dict(), path)\n","    print(f\"Model saved to {path}\")\n","\n","save_path = \"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/human-ness_model.pth\"\n","save_model(model, save_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"9qwgcTpbLIjH","executionInfo":{"status":"error","timestamp":1728917770264,"user_tz":240,"elapsed":682,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}},"outputId":"6eeacc5f-c286-44e2-e5ee-ea997060814e","collapsed":true},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X_train' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a16df0c3bbae>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}]},{"cell_type":"code","source":["import librosa\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","# Feature extraction\n","def extract_features(file_path):\n","    y, sr = librosa.load(file_path, sr=None)\n","    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n","    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n","    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n","    features = np.concatenate((mfccs.mean(axis=1), chroma.mean(axis=1), spectral_contrast.mean(axis=1)))\n","    return features\n","\n","# Model definition\n","class AudioRegressor(nn.Module):\n","    def __init__(self, input_dim):\n","        super(AudioRegressor, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, 2)\n","        self.fc4 = nn.Linear(2, 32)\n","        self.fc5 = nn.Linear(32, 64)\n","        self.fc6 = nn.Linear(64,1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.relu(self.fc3(x))\n","        x = self.relu(self.fc4(x))\n","        x = self.relu(self.fc5(x))\n","        x = self.fc6(x)\n","        return x\n","\n","# Load the model\n","def load_model(path, input_dim):\n","    model = AudioRegressor(input_dim)\n","    model.load_state_dict(torch.load(path))\n","    model.eval()  # Set the model to evaluation mode\n","    print(f\"Model loaded from {path}\")\n","    return model\n","\n","# Predict function\n","def predict(file_path, model):\n","    features = extract_features(file_path)\n","    features = torch.tensor(features, dtype=torch.float32).view(1, -1)\n","    model.eval()\n","    with torch.no_grad():\n","        prediction = model(features)  # Get the output tensor\n","        prediction = torch.mean(prediction).item()  # Example: Take the mean value\n","    return prediction\n","\n","# Example usage\n","save_path = \"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/human-ness_model.pth\"\n","input_dim = 32  # Number of features extracted\n","\n","# Load the model\n","loaded_model = load_model(save_path, input_dim)\n","\n","# Custom prediction\n","score = predict(\"/content/drive/MyDrive/MIDI4STRINGS_but_better/temp1.mp3\", loaded_model)\n","print(f\"The synthesized score of this custon audio is: {score:.2f}\")\n","\n","# Make prediction\n","for file in synthesized_files:\n","  score = predict(file, loaded_model)\n","  print(f\"File name: {file}\\nThe synthesized score of this synthesized audio is: {score:.2f}\")\n","for file in human_files:\n","  score = predict(file, loaded_model)\n","  print(f\"File name: {file}\\nThe synthesized score of this human audio is: {score:.2f}\")\n","# for i in range(10):\n","#   file_path = f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/MIDI_{i}.mp3\"\n","#   score = predict(file_path, loaded_model)\n","#   print(f\"The synthesized score of this synthesized audio is: {score:.2f} MIDI{i}\")\n","# for i in range(10):\n","#   file_path = f\"/content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human_{i}.mp3\"\n","#   score = predict(file_path, loaded_model)\n","#   print(f\"The synthesized score of this human audio is: {score:.2f} Human{i}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"p-Szj7fQVUZA","executionInfo":{"status":"error","timestamp":1724127558999,"user_tz":240,"elapsed":3722,"user":{"displayName":"Samuel Wang","userId":"13510211692320987499"}},"outputId":"0b52d1b3-30bd-4c21-f6a2-29d25efc3197"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from /content/drive/MyDrive/MIDI4STRINGS_but_better/Training Data/Human-ness Training Data/human-ness_model.pth\n","The synthesized score of this custon audio is: 1.01\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'synthesized_files' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6fc9a4ae941b>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynthesized_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File name: {file}\\nThe synthesized score of this synthesized audio is: {score:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'synthesized_files' is not defined"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"gyVsQEvZAK3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ws3L2LlpgXs_"},"execution_count":null,"outputs":[]}]}